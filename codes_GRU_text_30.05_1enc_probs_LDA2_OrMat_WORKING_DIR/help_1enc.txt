==========
What did we do in python:
==========

#17.05

import sys
import csv
import numpy as np
import theano
import theano.tensor as T
import lasagne
#import cPickle as pickle
import logging
from lasagne.layers import get_output
from lasagne.regularization import regularize_layer_params_weighted, l2, l1
import os
import linecache
from lasagne.layers import InputLayer, ExpressionLayer, EmbeddingLayer


logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
results_directory = 'Results_multivariate_text/'

def load_parameters2_mvrt_text(params_csvfile, line_num): 
    """
    reads the params for the new generated params
    """
    params={}
    values = linecache.getline(params_csvfile,line_num)[:-1].split(',')
    params['num_units'] = int(values[0])
    params['num_att_units'] = int(values[1])
    data_type = str(values[2]) 
    params['data_name'] = str(values[3])
    model = str(values[4]) 
    if params['data_name'] == 'coservit':
        params['windowise'] = 288
        params['horizon'] = 27
        params['word_dim'] = 200
        params['vocab_size'] = 155564
    if data_type == 'orig':
        params['data_type'] = 'original'
        if params['data_name'] == 'coservit':
            #params['time_data'] = '../../../Data' + params['data_name'] + 
            #params['text_data'] = '../../../Data' + params['data_name'] + 
            #params['data_file'] =  '../../../Data/' + params['data_name'] + 'ticket2time.pkl'
            params['data_file'] =  '../../../Data/coservit/' + 'x_enc.pkl'
            params['text_file_wv'] =  '../../../Data/coservit/' + 'x_dec_wv.dat' ##params['text_file_wv'] =  '../../../Data/coservit/' + 'tickets_wv_pad_mtx.dat'
            params['text_file_w'] =  '../../../Data/coservit/' + 'x_dec_w.dat' #params['text_file_w'] =  '../../../Data/coservit/' + 'tickets_w_pad_mtx.dat'
            params['metr_dec_file'] =  '../../../Data/coservit/' + 'metr_for_dec.pkl' #list of lists with metric ids corresponding to aligned tickets. Ex.: t1,t1,t2,t3,t3,t3->[[3432, 4657], [3442], [6567, 4657, 7855]]
        else:
            #TODO
            pass
    else:
        pass
        """params['data_type'] = 'missing'
        #params['data_file'] = '../../../Data/' + params['data_name'] + '/missing_gaps_v2/'+ params['data_name'] + '_'+ data_type +'.pkl'
        params['data_file'] = '../../../Data/' + params['data_name'] + '/missing_gaps_v2/'+ params['data_name'] + '_'+ data_type +'_intr_linear.pkl'
        params['data_file_missing_mask'] = '../../../Data/' + params['data_name'] + '/missing_gaps_v2/' + params['data_name'] + '_mask_' + data_type + '.pkl'
        params['missing_percent'] = data_type
        params['original_data_file'] = '../../../Data/' + params['data_name'] + '/original_and_interpolation/' + params['data_name'] +'.pkl'
        params['original_data_mask_file'] = '../../../Data/' + params['data_name'] + '/original_and_interpolation/' + params['data_name'] +'_mask.pkl' 
    #TODO:
    if params['data_name']  == 'pse':
        params['data2_name'] = 'weather_data'
        params['data2_file'] = '../../../Data/' + params['data2_name'] + '/missing_gaps_v2/'+ params['data2_name'] + '_'+ data_type +'.pkl'
        params['data2_file_missing_mask'] = '../../../Data/' + params['data_name'] + '/missing_gaps_v2/' + params['data_name'] + '_mask_' + data_type + '.pkl'
        params['data2_variables'] = ['mintmp', 'maxtmp', 'precip'] 
    elif params['data_name']  == 'consseason':
        params['data2_name'] = 'consseason'
        params['data2_file'] = params['data_file']
        params['data2_file_missing_mask'] = params['data_file_missing_mask']
        params['data2_variables'] = ['Global_reactive_power', 'Voltage', 'Global_intensity'] 
    elif params['data_name']  == 'airq':
        params['data2_name'] = 'airq'
        params['data2_file'] = params['data_file']
        params['data2_file_missing_mask'] = params['data_file_missing_mask']
        params['data2_variables'] = ['NO2(GT)', 'CO(GT)', 'NOx(GT)'] """
    if model == 'RNN':
        params['attention'] = False
    else:
        params['attention'] = True
        if model == 'RNN-ATT':
            params['att_type'] = 'original'
        elif model == 'ATT-lambda':
            params['att_type'] = 'lambda'
        elif model == 'ATT-lambda-mu':
            params['att_type'] = 'lambda_mu' 
        elif model == 'ATT-lambda-mu-alt':
            params['att_type'] = 'lambda_mu_alt'
        elif model == 'ATT-dist':
            params['att_type'] = 'adist'        
    params['alg'] = 'adam'
    params['lambda_regularization'] = 0.0001
    params['eta_decay'] = 0.95
    params['early_stop_patience'] = 10000
    params['num_layers'] = 1
    params['regularization_type'] = 'l2'
    params['early_stop'] = True
    params['grad_clipping'] = 100
    params['train_test_ratio'] = 0.75
    params['train_val_ratio'] = 0.75
    params['patience_increase'] = 2 
    params['epsilon_improvement'] = True 
    params['lr_decay_after_n_epoch'] = 50
    params['interpolation'] = True #!!!!!!
    params['max_num_epochs'] = 25
    params['epsilon'] = 0.01
    params['learning_rate'] = 0.001
    params['batch_size'] = 64
    params['padding'] = True 
    params['learning_rate_decay'] = False
    params['interpolation_type'] = 'None'
    return params
	
	
params = load_parameters2_mvrt_text('params/fuzzy.param.text', 1)
params


X_enc_sym = T.dtensor3('x_enc_sym')
X_dec_sym = T.dtensor3('x_dec_sym')
y_sym = T.lmatrix('y_sym') # indexes of 1hot words, for loss #y_sym = T.ftensor3('y_sym') #y_sym = T.matrix('y_sym')
Emb_mtx_sym = T.dmatrix('emb_mtx_sym') #(155564, 200)
eta = theano.shared(np.array(params['learning_rate'], dtype=theano.config.floatX))

word_dim = params['word_dim']
vocab_size = params['vocab_size']
grad_clip = params['grad_clipping']
hidden_size = params['num_units']
pred_len = 27
hidden_size=64
grad_clip = 100
vocab_size=155564
word_dim=200

#l_out, l_out_val = model_seq2seq_GRU_text(X_enc_sym, X_enc_sym2, X_dec_sym, params['windowise'], params['horizon'],  hidden_size = params['num_units'], 
#                                                            grad_clip = params['grad_clipping'], vocab_size = params['vocab_size'], word_dim = params['word_dim'])
														
														
l_in_enc = lasagne.layers.InputLayer(shape=(None, None, 1), input_var=X_enc_sym)
l_forward = lasagne.layers.GRULayer(l_in_enc, num_units=hidden_size, resetgate=lasagne.layers.Gate(W_in=lasagne.init.Uniform(), W_hid=lasagne.init.Uniform(), 		W_cell=lasagne.init.Uniform()), updategate=lasagne.layers.Gate(W_in=lasagne.init.Uniform(), W_hid=lasagne.init.Uniform(), W_cell=lasagne.init.Uniform()), 		hidden_update=lasagne.layers.Gate(W_in=lasagne.init.Uniform(), W_hid=lasagne.init.Uniform(), W_cell=lasagne.init.Uniform()), grad_clipping=grad_clip, only_return_final=True)
l_backward = lasagne.layers.GRULayer(l_in_enc, num_units=hidden_size, resetgate=lasagne.layers.Gate(W_in=lasagne.init.Uniform(), W_hid=lasagne.init.Uniform(), 		W_cell=lasagne.init.Uniform()), updategate=lasagne.layers.Gate(W_in=lasagne.init.Uniform(), W_hid=lasagne.init.Uniform(), W_cell=lasagne.init.Uniform()), grad_clipping=grad_clip, 		only_return_final=True, backwards=True)
l_enc = lasagne.layers.ConcatLayer([l_forward, l_backward], axis=1)
dec_units = 2*1 +1
l_in_dec = lasagne.layers.InputLayer(shape=(None, pred_len, word_dim),input_var=X_dec_sym) #(None, 27, 200)
s_lin_dec = lasagne.layers.SliceLayer(l_in_dec, indices=0, axis=1)
s_lin_dec = lasagne.layers.ReshapeLayer(s_lin_dec, ([0], 1, [1]))	#(None, 1, 200)
h_init = lasagne.layers.ConcatLayer([l_forward, l_enc], axis=1)
l_dec = lasagne.layers.GRULayer(s_lin_dec, num_units=hidden_size*dec_units, resetgate=lasagne.layers.Gate(W_in=lasagne.init.Uniform(), W_hid=lasagne.init.Uniform(), 		W_cell=lasagne.init.Uniform()), updategate=lasagne.layers.Gate(W_in=lasagne.init.Uniform(), W_hid=lasagne.init.Uniform(), W_cell=lasagne.init.Uniform()), 		hidden_update=lasagne.layers.Gate(W_in=lasagne.init.Uniform(), W_hid=lasagne.init.Uniform(), W_cell=lasagne.init.Uniform()), learn_init=False, hid_init=h_init, 		grad_clipping=grad_clip, only_return_final=True )
										 
r_gate = lasagne.layers.Gate(W_in=l_dec.W_in_to_resetgate, W_hid=l_dec.W_hid_to_resetgate, b=l_dec.b_resetgate)
u_gate = lasagne.layers.Gate(W_in=l_dec.W_in_to_updategate, W_hid=l_dec.W_hid_to_updategate, b=l_dec.b_updategate)
h_update = lasagne.layers.Gate(W_in=l_dec.W_in_to_hidden_update, W_hid=l_dec.W_hid_to_hidden_update, b=l_dec.b_hidden_update)
l_dec_hid_state = lasagne.layers.SliceLayer(l_dec, indices=slice(0,hidden_size)) #(None, 64)
l_out = lasagne.layers.DenseLayer(l_dec_hid_state, num_units=vocab_size, nonlinearity=lasagne.nonlinearities.softmax)

w_dense = l_out.W
b_dense = l_out.b
l_out_loop = l_out #(None, 155563)
l_out_loop_val = l_out #(None, 155563)
l_out = lasagne.layers.ReshapeLayer(l_out, ([0], 1, [1])) #3d (None, 1, 155563)
l_out_val = l_out #3d (None, 1, 155563)
h_init = lasagne.layers.ConcatLayer([l_dec_hid_state, l_enc], axis=1) # (None, 320)
h_init_val = lasagne.layers.ConcatLayer([l_dec_hid_state, l_enc], axis=1) #(None, 320) 

for i in range(1,pred_len): 
	#i=1
	s_lin_dec = lasagne.layers.SliceLayer(l_in_dec, indices=i, axis=1) #(None, 200)
  	s_lin_dec = lasagne.layers.ReshapeLayer(s_lin_dec, ([0], 1, [1])) #(None, 1, 200)
	l_dec = lasagne.layers.GRULayer(s_lin_dec, num_units=hidden_size*dec_units, learn_init=False,	 #(None, 320)
                                         hid_init=h_init, grad_clipping=grad_clip, only_return_final=True, 
                                         resetgate=r_gate, updategate=u_gate, hidden_update=h_update)
	l_dec_hid_state = lasagne.layers.SliceLayer(l_dec, indices=slice(0,hidden_size)) #(None, 64)
    
	#for val and train sets
	coord_max = ExpressionLayer(l_out_loop_val, lambda X: X.argmax(-1).astype('int32'), output_shape='auto') #(None,)
	emb_slice = ExpressionLayer(coord_max, lambda x: Emb_mtx_sym[x,:], output_shape=(None,word_dim)) #(None, 200)
	
	#coord_max = ExpressionLayer(l_out_loop_val, lambda X: X.argmax(-1).astype('int32'), output_shape='auto') #(None,)
	#pred_d = ExpressionLayer(coord_max, lambda X: T.set_subtensor(T.zeros((1,word_dim))[0,X], 1), output_shape='auto')
	#pred_emb = lasagne.layers.DenseLayer(pred_d, num_units=word_dim, W=Emb_mtx_sym, nonlinearity=lasagne.nonlinearities.linear) #no biases #(None, 200)
	#pred_emb.params[pred_emb.W].remove("trainable")
	"""
	def argm(l_row):
		return T.argmax(l_row)
	
	coord_max = theano.scan(fn=argm, outputs_info=None, sequences=l_out_loop_val, non_sequences=None)

	def get_row(a_coord, emb_mtx):
		return 1
	
	pred_d, updates = theano.scan(fn=get_row, outputs_info=None, sequences=coord_max, non_sequences=Emb_mtx_sym)
	func = theano.function(inputs=[coord_max], outputs=pred_d)
	"""
	
	#coord_max = ExpressionLayer(l_out_loop_val, lambda X: T.set_subtensor(T.zeros((1,word_dim))[0,X.argmax(-1).astype('int32')], 1), output_shape='auto')
	#hot = lasagne.layers.DenseLayer(coord_max, num_units=word_dim, nonlinearity=lasagne.nonlinearities.linear)
	
	pred = lasagne.layers.ReshapeLayer(emb_slice, ([0], 1, [1])) #######pred = lasagne.layers.ReshapeLayer(pred_emb, ([0], 1, [1])) #(None, 1, 200)
	
	l_dec_val = lasagne.layers.GRULayer(pred, num_units=hidden_size*dec_units, learn_init=False,
                                         hid_init=h_init_val, grad_clipping=grad_clip, only_return_final=True,
                                         resetgate=r_gate, updategate=u_gate, hidden_update=h_update) #(None, 320)
	l_dec_val_hid_state = lasagne.layers.SliceLayer(l_dec_val, indices=slice(0,hidden_size)) #(None, 64)
	l_out_loop = lasagne.layers.DenseLayer(l_dec_hid_state, num_units=vocab_size, W=w_dense, b=b_dense, nonlinearity=lasagne.nonlinearities.softmax) #(None, 155563)
	l_out_loop = lasagne.layers.ReshapeLayer(l_out_loop, ([0], 1, [1])) ####### #(None, 1, 155563)
	l_out = lasagne.layers.ConcatLayer([l_out, l_out_loop], axis=1) #(None, 2, 155563)
	l_out_loop_val = lasagne.layers.DenseLayer(l_dec_val_hid_state, num_units=vocab_size, W=w_dense, b=b_dense, nonlinearity=lasagne.nonlinearities.softmax) #(None, 155563)
	l_out_loop_val_d = lasagne.layers.ReshapeLayer(l_out_loop_val, ([0], 1, [1])) #(None, 1, 155563)
	l_out_val = lasagne.layers.ConcatLayer([l_out_val, l_out_loop_val_d], axis=1) #(None, 2, 155563)
	#l_out_loop_val = lasagne.layers.FlattenLayer(l_out_loop_val, 2)
	h_init = lasagne.layers.ConcatLayer([l_dec_hid_state, l_enc], axis=1) #(None, 320)
	h_init_val = lasagne.layers.ConcatLayer([l_dec_val_hid_state, l_enc], axis=1) #(None, 320)

#l_out.output_shape #shape = (None, 27, 155563)
#l_out_val.output_shape # shape = (None, 27, 155563)

#l_out = lasagne.layers.ReshapeLayer(l_out, (-1, hidden_size)) #(None, 64)
#l_out_val = lasagne.layers.ReshapeLayer(l_out_val, (-1, hidden_size)) #(None, 64)




===================================

l_out_conc = lasagne.layers.ConcatLayer([l_out, l_out_loop])
l_out_val_conc = lasagne.layers.ConcatLayer([l_out_val, l_out_loop_val], axis=1)

l_out = lasagne.layers.ReshapeLayer(l_out, ([0], 1, [1]))
l_out_loop = lasagne.layers.ReshapeLayer(l_out_loop, ([0], 1, [1]))



===================================

aa = np.array([[[1,2,3], [4,5,6], [0,2,1], [0,1,2]], [[6,7,8], [3,6,0], [4,3,2], [1,1,2]]])
a = theano.shared(aa, 'a')
bb = np.array([[1,1,1,1], [1,1,1,1]])
b = theano.shared(bb, 'b')

def categorical_crossentropy_3d(coding_dist, true_dist):
	# Zero out the false probabilities and sum the remaining true probabilities to remove the third dimension.
	indexes = theano.tensor.arange(coding_dist.shape[2])
	mask = theano.tensor.neq(indexes, true_dist.reshape((true_dist.shape[0], true_dist.shape[1], 1)))
	pred_probs = theano.tensor.set_subtensor(coding_dist[theano.tensor.nonzero(mask)], 0.).sum(axis=2)
	pred_probs_log = T.log(pred_probs)
	pred_probs_per_sample = -pred_probs_log.sum(axis=1)
	return pred_probs, pred_probs_log, pred_probs_per_sample

s, ss, sss = categorical_crossentropy_3d(a, b)
s, ss, sss = s.eval(), ss.eval(), sss.eval()
print(s)
print(ss)
print(sss)

===================================

#elementwise multiplication of 3d tensors
aa = np.array([[[1,2,3], [4,5,6], [0,2,1], [0,1,2]], [[6,7,8], [3,6,0], [4,3,2], [1,1,2]]], dtype=np.float32)
bb = np.array([[[0,0,2], [6,7,1], [0,-1,9], [0,9,-2]], [[6,7,8], [3,6,0], [4,3,2], [1,1,2]]], dtype=np.float32)

x = T.ftensor3('x')
y = T.ftensor3('y')
z = x * y
f1 = theano.function([x, y], z)
print(f1(aa,bb))

#try the same but with cross_entr function

a = theano.shared(aa, 'a')
b = theano.shared(bb, 'b')

def crossentropy_3d(coding_dist, true_dist):
	pred_probs = coding_dist * true_dist
	pred_probs_per_sample = pred_probs.sum(axis=2).sum(axis=1)
	return -theano.tensor.log(pred_probs_per_sample)
	
z = crossentropy_3d(a, b)
result = z.eval()
print(result)


===================================

Traceback (most recent call last):
  File "prediction_rnn_mvrt_intrl_GRU.py", line 2056, in <module>
    main()  
  File "prediction_rnn_mvrt_intrl_GRU.py", line 1989, in main
    train_loss, validation_loss = train_and_test_text(params, results_directory)
  File "prediction_rnn_mvrt_intrl_GRU.py", line 1865, in train_and_test_text
    val_loss = f_val(X_val, X_val2, X_val_dec, y_val_w) #val_loss = f_val(X_val, X_val_mask, X_val2, X_val_mask2, X_val3, X_val_mask3, X_val4, X_val_mask4, X_val_dec, X_val_mask_dec, y_val)
  File "/home/ama/kaznachm/.local/lib/python2.7/site-packages/theano/compile/function_module.py", line 898, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/home/ama/kaznachm/.local/lib/python2.7/site-packages/theano/gof/link.py", line 325, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/home/ama/kaznachm/.local/lib/python2.7/site-packages/theano/compile/function_module.py", line 884, in __call__
    self.fn() if output_subset is None else\
ValueError: Shape mismatch: x has 155563 cols (and 260 rows) but y has 200 rows (and 1920 cols)
Apply node that caused the error: Dot22(SoftmaxWithBias.0, Join.0)
Toposort index: 285
Inputs types: [TensorType(float64, matrix), TensorType(float64, matrix)]
Inputs shapes: [(260, 155563), (200, 1920)]
Inputs strides: [(1244504, 8), (15360, 8)]
Inputs values: ['not shown', 'not shown']
Outputs clients: [[Reshape{2}(Dot22.0, MakeVector{dtype='int64'}.0)]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.

===================================


for i in range(1,pred_len): 
	s_lin_dec = lasagne.layers.SliceLayer(l_in_dec, indices=i, axis=1) #(None, 200)
  	s_lin_dec = lasagne.layers.ReshapeLayer(s_lin_dec, ([0], 1, [1])) #(None, 1, 200)
	l_dec = lasagne.layers.GRULayer(s_lin_dec, num_units=hidden_size*dec_units, learn_init=False,	 #(None, 320)
                                         hid_init=h_init, grad_clipping=grad_clip, only_return_final=True, 
                                         resetgate=r_gate, updategate=u_gate, hidden_update=h_update)
	l_dec_hid_state = lasagne.layers.SliceLayer(l_dec, indices=slice(0,hidden_size)) #(None, 64)
	pred = lasagne.layers.ReshapeLayer(l_out_loop_val, ([0], 1, [1])) #(None, 1, 155563)
	pred_2 = lasagne.layers.DenseLayer(pred, num_units=word_dim, W=w_dense, b=b_dense, nonlinearity=lasagne.nonlinearities.softmax) #(None, 155563)
	pred_2 = lasagne.layers.ReshapeLayer(pred_2, ([0], 1, [1])) ####### #(None, 1, 155563)
    
	#ind_word = T.argmax(pred, axis=0)
	l_dec_val = lasagne.layers.GRULayer(pred_2, num_units=hidden_size*dec_units, learn_init=False,
                                         hid_init=h_init_val, grad_clipping=grad_clip, only_return_final=True,
                                         resetgate=r_gate, updategate=u_gate, hidden_update=h_update) #(None, 320)
	l_dec_val_hid_state = lasagne.layers.SliceLayer(l_dec_val, indices=slice(0,hidden_size)) #(None, 64)
	l_out_loop = lasagne.layers.DenseLayer(l_dec_hid_state, num_units=vocab_size, W=w_dense, b=b_dense, nonlinearity=lasagne.nonlinearities.softmax) #(None, 155563)
	l_out_loop = lasagne.layers.ReshapeLayer(l_out_loop, ([0], 1, [1])) ####### #(None, 1, 155563)
	l_out = lasagne.layers.ConcatLayer([l_out, l_out_loop], axis=1) #(None, 2, 155563)
	l_out_loop_val = lasagne.layers.DenseLayer(l_dec_val_hid_state, num_units=vocab_size, W=w_dense, b=b_dense, nonlinearity=lasagne.nonlinearities.softmax) #(None, 155563)
	l_out_loop_val_d = lasagne.layers.ReshapeLayer(l_out_loop_val, ([0], 1, [1])) #(None, 1, 155563)
	l_out_val = lasagne.layers.ConcatLayer([l_out_val, l_out_loop_val_d], axis=1) #(None, 2, 155563)
	#l_out_loop_val = lasagne.layers.FlattenLayer(l_out_loop_val, 2)
	h_init = lasagne.layers.ConcatLayer([l_dec_hid_state, l_enc], axis=1) #(None, 320)
	h_init_val = lasagne.layers.ConcatLayer([l_dec_val_hid_state, l_enc], axis=1) #(None, 320)
	
	
	
===================================

01.06.17 Order matters

import lasagne
import numpy as np
import theano
import theano.tensor as T
from lasagne import nonlinearities
from lasagne import init
from lasagne.utils import unroll_scan
from lasagne.layers.base import Layer, MergeLayer
from lasagne.layers.input import InputLayer
from lasagne.layers.dense import DenseLayer
from lasagne.layers import helper, SliceLayer
from lasagne.layers.recurrent import Gate

class GRULayer_setenc(MergeLayer):
    def __init__(self, num_units, #incoming, num_units, #---
                 resetgate=Gate(W_cell=None), #-- myGate() ??????
                 updategate=Gate(W_cell=None),
                 hidden_update=Gate(W_cell=None,
                 nonlinearity=nonlinearities.tanh),
                 hid_init=init.Constant(0.),
                 contxt_input= init.Constant(0.),#---
                 #ctx_init = init.Constant(0.), #---
                 att_num_units = 64,
                 W_hid_to_att = init.GlorotNormal(),
                 W_ctx_to_att = init.GlorotNormal(),
                 W_att = init.Normal(0.1), #---
                 backwards=False,
                 learn_init=True, #--???????
                 gradient_steps=-1,
                 grad_clipping=False,
                 unroll_scan=False,
                 precompute_input=True,
                 mask_input=None,
                 #only_return_final=True, #-- ???????
                 **kwargs):
	super(GRULayer, self).__init__(incomings, **kwargs)
	self.learn_init = learn_init
	self.num_units = num_units
	self.grad_clipping = grad_clipping
	self.backwards = backwards
	self.gradient_steps = gradient_steps
	self.unroll_scan = unroll_scan
	self.precompute_input = precompute_input
	if unroll_scan and gradient_steps != -1:
		raise ValueError("Gradient steps must be -1 when unroll_scan is true.")
	input_shape = self.input_shapes[0]
	if unroll_scan and input_shape[1] is None:
		raise ValueError("Input sequence length cannot be specified as "
		"None when unroll_scan is True")
	num_inputs = np.prod(input_shape[2:])
	def add_gate_params(gate, gate_name):
		""" Convenience function for adding layer parameters from a Gate
		instance. """
		return (self.add_param(gate.W_in, (num_inputs, num_units),
                               name="W_in_to_{}".format(gate_name)),
                self.add_param(gate.W_hid, (num_units, num_units),
                               name="W_hid_to_{}".format(gate_name)),
                self.add_param(gate.b, (num_units,),
                               name="b_{}".format(gate_name),
                               regularizable=False),
                gate.nonlinearity)
	(self.W_in_to_updategate, self.W_hid_to_updategate, self.b_updategate,
		self.nonlinearity_updategate) = add_gate_params(updategate, 'updategate')
	(self.W_in_to_resetgate, self.W_hid_to_resetgate, self.b_resetgate,
		self.nonlinearity_resetgate) = add_gate_params(resetgate, 'resetgate')
	(self.W_in_to_hidden_update, self.W_hid_to_hidden_update,
		self.b_hidden_update, self.nonlinearity_hid) = add_gate_params(hidden_update, 'hidden_update')
	if isinstance(hid_init, T.TensorVariable):
		if hid_init.ndim != 2:
			raise ValueError("When hid_init is provided as a TensorVariable, it should "
							"have 2 dimensions and have shape (num_batch, num_units)")
		self.hid_init = hid_init
	else:
		self.hid_init = self.add_param(
			hid_init, (1, self.num_units), name="hid_init",
            trainable=learn_init, regularizable=False)
	def get_output_shape_for(self, input_shapes):
		input_shape = input_shapes[0]
		return input_shape[0], input_shape[1], self.num_units
	def get_output_for(self, inputs, **kwargs):
		input = inputs[0]
		# Retrieve the mask when it is supplied
		mask = inputs[1] if len(inputs) > 1 else None
	if input.ndim > 3:
		input = T.flatten(input, 3)
	input = input.dimshuffle(1, 0, 2)
	seq_len, num_batch, _ = input.shape
	W_in_stacked = T.concatenate(
		[self.W_in_to_resetgate, self.W_in_to_updategate,
         self.W_in_to_hidden_update], axis=1)
	W_hid_stacked = T.concatenate(
		[self.W_hid_to_resetgate, self.W_hid_to_updategate,
         self.W_hid_to_hidden_update], axis=1)
	b_stacked = T.concatenate(
        [self.b_resetgate, self.b_updategate,
         self.b_hidden_update], axis=0)
	if self.precompute_input:
		input = T.dot(input, W_in_stacked) + b_stacked
	def slice_w(x, n):
		return x[:, n*self.num_units:(n+1)*self.num_units]
	def plain_et_step(self, x_snp, o_t0):
		m_in = x_snp.dimshuffle(1, 0, 2)
		e_qt = T.dot(o_t0, self.W_aq)
		e_m = T.dot(m_in, self.W_am)
		e_q = T.tile(e_qt, (self.seq_len_m, 1, 1))
		et_p = T.tanh(e_m + e_q + self.b_a)
		et = T.dot(et_p, self.W_a)
		alpha = T.exp(et)
		alpha /= T.sum(alpha, axis=0)
		mt = x_snp.dimshuffle(2, 1, 0)
		mult = T.mul(mt, alpha)
		rt = T.sum(mult, axis=1)
		return rt.T
	def step(x_snp, snp_mask, m_snp_count):
		r_t = T.nnet.sigmoid(T.dot(hr_tm1, self.Wo_hh_r) + self.bo_r)
		z_t = T.nnet.sigmoid(T.dot(hr_tm1, self.W_hh_z) + self.b_z)
		h_tilde = T.tanh(T.dot(r_t * hr_tm1, self.W_hh) + self.b_hh)
		o_t = (np.float32(1.0) - z_t) * hr_tm1 + z_t * h_tilde
		rt = self.plain_et_step(x_snp, o_t)
		h_t = T.concatenate([o_t0,rt], axis=1)
		return h_t
	def step_masked(input_n, mask_n, hid_previous, W_hid_stacked, W_in_stacked, b_stacked):
		hid = step(input_n, hid_previous, W_hid_stacked, W_in_stacked, b_stacked)
		not_mask = 1 - mask_n
		hid = hid*mask_n + hid_previous*not_mask
		return hid
	if mask is not None:
		mask = mask.dimshuffle(1, 0, 'x')
		sequences = [input, mask]
		step_fun = step_masked
	else:
		sequences = [input]
		step_fun = step
	if isinstance(self.hid_init, T.TensorVariable):
		hid_init = self.hid_init
	else:
		hid_init = T.dot(T.ones((num_batch, 1)), self.hid_init)
	non_seqs = [W_hid_stacked]
	if not self.precompute_input:
		non_seqs += [W_in_stacked, b_stacked]
	else:
		non_seqs += [(), ()]
	if self.unroll_scan:
		input_shape = self.input_shapes[0]
		hid_out = unroll_scan(
            fn=step_fun,
            sequences=sequences,
            outputs_info=[hid_init],
            go_backwards=self.backwards,
            non_sequences=non_seqs,
            n_steps=input_shape[1])[0]
	else:
		hid_out, _ = theano.scan(seq_enc_step, outputs_info=o_enc_info, sequences=[xmask], non_sequences=[x_snp, snp_mask, m_snp_count], n_steps=self.num_set_iter)
		"""hid_out = theano.scan(
            fn=step_fun,
            sequences=sequences,
            go_backwards=self.backwards,
            outputs_info=[hid_init],
            non_sequences=non_seqs,
            truncate_gradient=self.gradient_steps,
            strict=True)[0]"""
	hid_out = hid_out.dimshuffle(1, 0, 2)
	if self.backwards:
		hid_out = hid_out[:, ::-1, :]
	return hid_out
	
	
	
	


